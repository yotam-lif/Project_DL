{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from dataloader import TFDNA_ds\n",
    "import torch\n",
    "import os\n",
    "import helper_classes as hp\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T12:32:13.895125Z",
     "start_time": "2023-08-27T12:32:12.056023Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9146cd8-a590-45b8-8b16-b04abaf8745f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-27T12:32:14.941192Z",
     "start_time": "2023-08-27T12:32:14.930865Z"
    }
   },
   "outputs": [],
   "source": [
    "n_layers = 5\n",
    "d_model = 80\n",
    "n_heads = 1\n",
    "dropout_enc = 0.1\n",
    "window_size = 300\n",
    "dropout_emb = True\n",
    "slide_step = 200\n",
    "n_epochs = 10\n",
    "figs = []\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4332736c-7906-41ec-a405-1fff955f546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows the access to the files\n",
    "cwd = os.getcwd()\n",
    "os.chdir('/home/labs/antebilab/naamab_lab/Benny/Deep_learning_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf64e27aa113c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T17:35:17.157961Z",
     "start_time": "2023-08-15T17:35:17.129811Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae0748aea9de76",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF_path = 'Amino_acid_data/final_tf_data.h5'\n",
    "DNA_path = 'signal_159_TFs'\n",
    "data = TFDNA_ds(TF_path=TF_path, DNA_path=DNA_path, seq_length=window_size, sliding_window_step=slide_step)\n",
    "\n",
    "# N_train = int(len(data)*10e-5)\n",
    "# training_dataset = torch.utils.data.Subset(data,range(N_train))\n",
    "# validation_dataset = torch.utils.data.Subset(data,range(N_train,len(data)))\n",
    "\n",
    "N_train = (10 ** 4) * 5\n",
    "N_valid = int(N_train / 10)\n",
    "training_dataset = torch.utils.data.Subset(data,range(N_train))\n",
    "validation_dataset = torch.utils.data.Subset(data,range(N_train, N_train + N_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5cd5da9dd27e25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import EncoderNet, FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe0c36e3873df79",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# net = EncoderNet(n_layers=n_layers, d_model=d_model, n_heads=n_heads, dropout_enc=dropout_enc, window_size=window_size, dropout_emb=dropout_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e954778-1857-4bb8-9e6b-5be0f5a5fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1502 + 1 + window_size\n",
    "N_hidden = [input_size * 2, input_size * 4, input_size * 4, input_size * 2]\n",
    "net = FCN(N_input=input_size, N_hidden=N_hidden, N_output=window_size, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557a0276-ea36-40a3-ad67-e73192d2c40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (node_network): Sequential(\n",
       "    (0): Linear(in_features=1803, out_features=3606, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(3606, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=3606, out_features=7212, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(7212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=7212, out_features=7212, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm1d(7212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "    (12): Linear(in_features=7212, out_features=3606, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=3606, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a9138a-3110-44d7-9686-ca16c4625697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "N_GPUs = torch.cuda.device_count()\n",
    "if N_GPUs > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "    batch_size = batch_size * N_GPUs\n",
    "print(N_GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c3666c-5a83-423f-9758-381ce91c77f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-27T12:11:18.398277Z",
     "start_time": "2023-08-27T12:11:15.408972Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, DataLoader\n\u001B[0;32m----> 3\u001B[0m data_loader \u001B[38;5;241m=\u001B[39m DataLoader(\u001B[43mtraining_dataset\u001B[49m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m validation_data_loader \u001B[38;5;241m=\u001B[39m DataLoader(validation_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'training_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_data_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf02468e-6d07-4814-83d5-f1ae3b71884d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1803, 26])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batched_e,l in data_loader:\n",
    "    break\n",
    "batched_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e94c7eb1d1ce56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (node_network): Sequential(\n",
       "    (0): Linear(in_features=1803, out_features=3606, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(3606, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=3606, out_features=7212, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(7212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=7212, out_features=7212, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm1d(7212, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "    (12): Linear(in_features=7212, out_features=3606, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=3606, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('cuda')\n",
    "print(device)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef1b1b915f9bf8af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_func = hp.Custom_MSE_Loss(c=4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c93a1de9531e5e5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_and_loss(dataloader, net, valid=False):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    \n",
    "    rnd_b = random.randint(1, int(N_valid/batch_size))\n",
    "    rnd_i = random.randint(1, batch_size)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_e, l in dataloader:\n",
    "            n_batches += 1\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                batched_e = batched_e.to(torch.device('cuda'))\n",
    "                l = l.cuda()\n",
    "            pred = net(batched_e)\n",
    "\n",
    "            loss += loss_func(pred, l).item()\n",
    "            \n",
    "            if valid and n_batches == rnd_b:\n",
    "                figs.append((pred[rnd_i], l[rnd_i], n_batches, rnd_i))\n",
    "\n",
    "    loss = loss / n_batches      \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e239f03-debe-45d2-b8d1-8e67ea39a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /home/labs/barkailab/benjak/.local/lib/python3.8/site-packages (8.1.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/labs/barkailab/benjak/.local/lib/python3.8/site-packages (from ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipywidgets) (8.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/labs/barkailab/benjak/.local/lib/python3.8/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/labs/barkailab/benjak/.local/lib/python3.8/site-packages (from ipywidgets) (4.0.8)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: pickleshare in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (50.3.1.post20201107)\n",
      "Requirement already satisfied: backcall in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: decorator in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /apps/easybd/easybuild/software/Miniconda3/4.9.2/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.15.0)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60701b005a06e56d",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4721256802b84eeaa7216ab62ce0a8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (36060x26 and 1803x3606)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m     l \u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 26\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatched_e\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(pred, l)\n\u001B[1;32m     28\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Python/Deeplearning/project/model.py:40\u001B[0m, in \u001B[0;36mFCN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (36060x26 and 1803x3606)"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "training_loss_vs_epoch = []\n",
    "validation_loss_vs_epoch = []\n",
    "\n",
    "# training_acc_vs_epoch = []\n",
    "# validation_acc_vs_epoch = []\n",
    "\n",
    "pbar = tqdm( range(n_epochs) )\n",
    "os.chdir(cwd)\n",
    "\n",
    "for epoch in pbar:\n",
    "    \n",
    "    if len(validation_loss_vs_epoch) > 1:\n",
    "        pbar.set_description('val loss:'+'{0:.5f}'.format(validation_loss_vs_epoch[-1])+', train loss:'+'{0:.5f}'.format(training_loss_vs_epoch[-1]))\n",
    "       \n",
    "    net.train() # put the net into \"training mode\"\n",
    "    for batched_e, l in data_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            batched_e = batched_e.to(torch.device('cuda'))\n",
    "            l = l.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        pred = net(batched_e)\n",
    "        loss = loss_func(pred, l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    net.eval() #put the net into evaluation mode\n",
    "    train_loss = compute_accuracy_and_loss(data_loader, net)\n",
    "    valid_loss =  compute_accuracy_and_loss(validation_data_loader, net, valid=True)\n",
    "    print(valid_loss)\n",
    "         \n",
    "    training_loss_vs_epoch.append(train_loss)    \n",
    "    # training_acc_vs_epoch.append( train_acc)\n",
    "    # validation_acc_vs_epoch.append(valid_acc)\n",
    "    validation_loss_vs_epoch.append(valid_loss)\n",
    "\n",
    "    #keep model with best loss\n",
    "    if valid_loss < best_loss:\n",
    "        torch.save(net.state_dict(), 'trained_model.pt')\n",
    "        best_loss = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd280635db3de58a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(training_loss_vs_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c4ef9-d09f-4cfd-a9b0-cf802ce61335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_loss_vs_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadd339d41c6590",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(training_loss_vs_epoch,label='training')\n",
    "plt.plot(validation_loss_vs_epoch,label='validation')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for e in figs:\n",
    "    pred = e[0].numpy(force=True)\n",
    "    label = e[1].numpy(force=True)\n",
    "    batch = e[2]\n",
    "    ind = e[3] \n",
    "    plt.plot(pred, label='pred')\n",
    "    plt.plot(label, label='label')\n",
    "    plt.title('batch number ' + str(batch) + ', example number ' + str(ind))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56f6af95-984b-46a4-a94e-dc33e9d62a5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92a985-1aa0-469d-b3ac-5c166df2a8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
